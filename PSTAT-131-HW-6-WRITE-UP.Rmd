---
title: "PSTAT 131 HW 6 WRITE UP"
author: "Katherine Bayt"
date: '2022-05-18'
output: html_document
---

```{r setup, include=FALSE}
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(corrplot)
library(ggthemes)
tidymodels_prefer()
library(ISLR)
library(yardstick)
library(corrr)
library(discrim)
library(poissonreg)
library(klaR)
library(janitor)
library(randomForest)
library(vip)
library(rpart.plot)
library(xgboost)
library(ranger)
set.seed(4857)
knitr::opts_chunk$set(echo = TRUE)
```


## EXERCISE 1
```{r}
# read in data
pokemon <- read.csv("C:\\Pokemon.csv")
# clean names
pokemon <- pokemon %>% clean_names
view(pokemon)
# filter out rarer types 
pokemon <- pokemon %>%
  filter(type_1 == "Bug" | type_1 == "Fire" |
           type_1 == "Grass" | type_1 == "Normal" |
           type_1 == "Water" | type_1 == "Psychic")
# convert type_1 and legendary to factors
pokemon$type_1 <- factor(pokemon$type_1)
pokemon$legendary <- factor(pokemon$legendary)
# inital split on data, stratify on type_1
pokemon_split <- initial_split(pokemon, prop = 0.7,
                               strata = type_1)
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)
# 318
dim(pokemon_train)
# 140
dim(pokemon_test)
# v-fold cross validation with v=5
pokemon_folds <- vfold_cv(pokemon_train, v=5, 
                          strata = type_1)
# recipe
pokemon_recipe <- recipe(type_1 ~ legendary +
                           generation + sp_atk +
                           attack + speed + defense +
                           hp + sp_def, data = pokemon_train) %>%
  step_dummy(legendary, generation) %>%
  step_normalize(all_predictors())

```

## EXERCISE 2
When setting up the matrix, I decided to remove the variable total, because it is the sum of all Pokemon stats, and thus would have a correlation of 1 with all the stats. I also decided to remove the variable x because it is the ID for each Pokemon and does not contain any influential data for the model. 
```{r}
# correlation matrix using corrplot 
pokemon %>%
  select(where(is.numeric)) %>%
  select(-c(x, total)) %>%
  cor() %>%
  corrplot(type = 'lower', diag = FALSE)
```
We can see that generation has no correlation (neither positive or negative) with any of the other numerical variables. We can see that all the stats are somewhat positively correlated with each other. It makes sense that defense and sp_def have the highest positive correlation out of all the variables because defense is the base damage resistance against normal attacks and sp_def is the base damage resistance against special attacks. Overall the positive correlations between all the numeric variables makes sense because the more a Pokemon is upgraded, the more well rounded their defense and attacking mechanisms become. 

## EXERCISE 3
```{r}
## create general decision treep specification
tree_spec <- decision_tree() %>%
  set_engine("rpart")
# create classification decision tree spec
pokemon_tree_spec <- tree_spec %>%
  set_mode("classification")
# set up workflow + tune cost complexity 
pokemon_tree_wf <- workflow() %>%
  add_model(pokemon_tree_spec %>% 
              set_args(cost_complexity = tune())) %>%
  add_recipe(pokemon_recipe)
# tune the hyper parameter cost complexity 
set.seed(4857)
param_grid <- grid_regular(cost_complexity(
  range=c(-3,-1)), levels = 10)

tune_res <- tune_grid(
  pokemon_tree_wf,
  resamples = pokemon_folds,
  grid = param_grid,
  metrics = metric_set(roc_auc)
)

# plot results
autoplot(tune_res)
```
From the plot we observe that a single decision tree performs better with a smaller complexity penalty compares to a larger. It appears that our best roc_auc value occurs at around .03 cost-complexity. 

## EXERCISE 4
```{r}
# roc_auc of best performing pruned decision tree
collect_metrics(tune_res) %>% arrange(-mean) %>% head()
best_complexity <- select_best(tune_res, metric = "roc_auc")
```
The roc_auc value of our best performing model 0.653. 

## EXERCISE 5
```{r}
# fit and visualize best-performing pruned decision tree with training set
pokemon_tree_final <- finalize_workflow(
  pokemon_tree_wf, best_complexity)

pokemon_tree_final_fit <- fit(pokemon_tree_final,
                              data = pokemon_train)
pokemon_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```
## EXERCISE 5 CONT
```{r}
rf_spec <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wkflow <- workflow() %>%
  add_model(rf_spec %>%
              set_args(mtry = tune(), trees = tune(),
                       min_n = tune())) %>%
  add_recipe(pokemon_recipe)


set.seed(4857)
multi_param_grid <- grid_regular(
  mtry(range = c(1,8)), trees(range(1,200)),
  min_n(range(1,30)), levels = 8)
multi_tune_res <- tune_grid(
  rf_wkflow,
  resamples = pokemon_folds,
  grid = multi_param_grid,
  metrics = metric_set(roc_auc))
```
According to the documentation of rand_forest(), mtry represents the number of predictors that will be used when creating the tree models. Mtry should not be less than 1 because we want at least one parameter to train our model, and we can not have a negative amount of parameters. Mtry should also not be greater than 8 because we only have 8 predictors in our model. A model with mtry = 8 would be a bagging model. Also, according to the documentation, trees represents the number of trees and min_n represents the minimum number of data points in a node that are required for the node to be split further. 

## EXERCISE 6
```{r}
autoplot(multi_tune_res)
```
From the plot is appears that higher values of trees perform better. In adition, higher minimal node sizes appeared to do better as well. The number of predictors seems to be the best when its four or greater. Looking at all the graphs, I think our optimal point is at minimal node size 13, 200 trees, and 2 predictors. 

## EXERCISE 7
```{r}
# best performing model roc_auc value
collect_metrics(multi_tune_res) %>% arrange(-mean) %>% head()
best_model <- select_best(multi_tune_res, 
                          metric = "roc_auc")

```
The roc_auc value of our best model is 0.743. 

## EXERCISE 8
```{r}
# fit to training et 
best_model_final <- finalize_workflow(rf_wkflow,
                                      best_model)
best_model_final_fit <- fit(best_model_final,
                            data = pokemon_train)
# vip() plot
best_model_final_fit %>% extract_fit_engine() %>% vip()
```
The most useful variables were sp_atk, hp, attack, and speed. The least useful variables were legendary_True, generation, defense, and sp_def. The most useful variables make sense to me, because (and I might be wrong because I have not played Poekmon is a while) the attacking characteristic of a Pokemon more greatly define their type compared to the defensive characteristics. 

## EXERCISE 9
```{r}
# Set up boosted tree model and worflow 
boost_spec <- boost_tree() %>%
  set_engine("xgboost") %>%
  set_mode("classification")
boost_wkflow <- workflow() %>%
  add_model(boost_spec %>%
              set_args(trees = tune())) %>%
  add_recipe(pokemon_recipe)
# create regular grid with 10 levels
trees_grid <- grid_regular(trees(range=c(10,2000)), 
                           levels = 10)

tree_tune_res <- tune_grid(
  boost_wkflow,
  resamples = pokemon_folds,
  grid = trees_grid,
  metrics = metric_set(roc_auc))
# plot results
autoplot(tree_tune_res)
```
Looking at the plot, it appears that the roc_auc value is best when the amount of trees is smaller, around 0-100. Our optimal roc_auc value is around 0-10 trees. 
```{r}
# best performing model
collect_metrics(tree_tune_res) %>% arrange(-mean) %>% head()
best_boost_trees <- select_best(tree_tune_res)
best_boost_model <- finalize_workflow(boost_wkflow,
                                      best_boost_trees)
best_boost_model_fit <- fit(best_boost_model,
                            data = pokemon_train)
```
The roc_auc value of our best model is 0.70.

## EXERCISE 10

```{r}
# pruned tree, rf, and boosted tree
pruned_tree_roc_auc <- collect_metrics(tune_res) %>% arrange(-mean) %>% head()
rf_roc_auc <- collect_metrics(multi_tune_res) %>% arrange(-mean) %>% head()
boost_roc_auc <- collect_metrics(tree_tune_res) %>% arrange(-mean) %>% head()

roc_auc_vals <- c(pruned_tree_roc_auc$mean[1], rf_roc_auc$mean[1], boost_roc_auc$mean[1])
models <- c("Pruned Tree", "Random Forest", "Boosted Tree")
total_res <- tibble(roc_auc = roc_auc_vals,
                    models = models)
total_res
```
The random forest model performed the best on the folds. 
```{r}
# fitting boosted model to testing data 
best_boost_trees_test <- select_best(tree_tune_res)
best_boost_model_test <- finalize_workflow(boost_wkflow,
                                      best_boost_trees)
best_boost_model_fit_test <- fit(best_boost_model_test,
                            data = pokemon_test)
```
```{r}
# print AUC value of boosted model 
augment(best_boost_model_fit_test, new_data = pokemon_test) %>%
  roc_auc(truth = type_1, estimate = c(.pred_Bug, 
                                       .pred_Fire, .pred_Grass, .pred_Normal,
                                       .pred_Psychic, .pred_Water))
```
```{r}
# ROC curves 
augment(best_boost_model_fit_test, new_data = pokemon_test) %>%
  roc_curve(truth = type_1, estimate = c(.pred_Bug, 
                                       .pred_Fire, .pred_Grass, .pred_Normal,
                                       .pred_Psychic, .pred_Water)) %>%
  autoplot()
```
```{r}
# confusion matrix heat map 
```

